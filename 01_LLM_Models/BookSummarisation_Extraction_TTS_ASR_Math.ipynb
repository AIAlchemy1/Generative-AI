{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AIAlchemy1/Generative-AI/blob/main/01_LLM_Models/BookSummarisation_Extraction_TTS_ASR_Math.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xwQF4AcAXq20"
      },
      "source": [
        "# Task 1. Book summarisation\n",
        "\n",
        "3 points\n",
        "\n",
        "OpenAI API is a great tool for summarization, but it is able to process only relatively short texts, as prescribed by the MAX TOKENS restrictions of the models. So, what if we want to summarize a whole book? Let's try to make a workaround for this.\n",
        "\n",
        "To test our solutions we will be using CMU dataset for book summarization. Let's start with downloading a sample of book dataset from huggingface\n",
        "\n",
        "```\n",
        "@article{kryscinski2021booksum,\n",
        "      title={BookSum: A Collection of Datasets for Long-form Narrative Summarization},\n",
        "      author={Wojciech Kry{\\'s}ci{\\'n}ski and Nazneen Rajani and Divyansh Agarwal and Caiming Xiong and Dragomir Radev},\n",
        "      year={2021},\n",
        "      eprint={2105.08209},\n",
        "      archivePrefix={arXiv},\n",
        "      primaryClass={cs.CL}\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NMDjvJisXq22",
        "outputId": "3866158e-a7f6-4e7a-f5e1-dc26a3f894eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 3771k  100 3771k    0     0  4973k      0 --:--:-- --:--:-- --:--:-- 4968k\n"
          ]
        }
      ],
      "source": [
        "!curl -X GET \\\n",
        "     \"https://datasets-server.huggingface.co/rows?dataset=kmfoda%2Fbooksum&config=default&split=train&offset=0&limit=100\" > book_sum_dataset.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wnXLVcVyXq23"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "book_dataset = json.load(open(\"book_sum_dataset.txt\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's look at one row of our database. Beware: a large text will get printed."
      ],
      "metadata": {
        "id": "uFU5jSBgbUM0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8XbdYE1ZXq23",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5300b39-5c23-4523-a8a3-55baa9268f6c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'row_idx': 0,\n",
              " 'row': {'bid': 27681,\n",
              "  'is_aggregate': True,\n",
              "  'source': 'cliffnotes',\n",
              "  'chapter_path': 'all_chapterized_books/27681-chapters/chapters_1_to_2.txt',\n",
              "  'summary_path': 'finished_summaries/cliffnotes/The Last of the Mohicans/section_1_part_0.txt',\n",
              "  'book_id': 'The Last of the Mohicans.chapters 1-2',\n",
              "  'summary_id': 'chapters 1-2',\n",
              "  'content': None,\n",
              "  'summary': '{\"name\": \"Chapters 1-2\", \"url\": \"https://web.archive.org/web/20201101053205/https://www.cliffsnotes.com/literature/l/the-last-of-the-mohicans/summary-and-analysis/chapters-12\", \"summary\": \"Before any characters appear, the time and geography are made clear. Though it is the last war that England and France waged for a country that neither would retain, the wilderness between the forces still has to be overcome first. Thus it is in 1757, in the New York area between the head waters of the Hudson River and Lake George to the north. Because only two years earlier General Braddock was disgracefully routed by a handful of French and Indians, the frontier is now exposed to real and imaginary savage disasters as well as to the horrors of warfare. Fear has replaced reason. Near dusk of a day in July, an Indian runner named Magua arrives at Fort Edward on the upper Hudson. He has come from Fort William Henry at the southern tip of Lake George with the news that the French General Montcalm is moving south with a very large army and that Munro, commander of Fort William Henry, is in urgent need of plentiful reinforcements from General Webb. Early the next morning, a limited detachment of fifteen hundred regulars and colonists departs as if swallowed by the forest. Shortly afterwards, Major Duncan Heyward and Alice and Cora Munro, guided by Magua on foot, take by horseback a secret route toward William Henry for the girls to join their father. Blonde Alice is doubtful about Magua, covered with war paint and showing a sullen fierceness; but dark-haired Cora is stoically common sense about him, even though Heyward mentions that their father had once had to deal rigidly with the Indian. As the small party pushes on, they are overtaken by David Gamut, a tall, ungainly psalmodist ridiculously dressed and carrying a pitch pipe while riding a mare followed by its young colt. He desires to join them, and after some banter between him and Alice, he pulls out the twenty-sixth edition of The Bay Psalm Book, sounds his pipe, and renders a song \\\\\"in full, sweet, and melodious tones.\\\\\" At a muttered comment from Magua, Heyward insists upon silence for safety. Then he glances about them and, satisfied that he has seen only shining berries, smiles to himself as they move on. But he is wrong. The branches move and a man peers exultingly after them as they disappear among the dark lines of trees.\", \"analysis\": \"These two chapters introduce the reader to the historical and natural settings and are indicative of the extent to which this book, as a historical novel, relates its fictional characters to real history. Only here at the beginning and later at mid-novel will the action coincide in detail with actual events, though the historic war is always somewhere in the near distance. These chapters also present four of the main fictional characters and one secondary one, all of whom will merit our concern henceforth. Major Heyward is the gallant romantic hero, but unlike most sentimental romances where for each hero there is one heroine, here there are two, Alice and Cora, blonde and brunette. And it is immediately apparent that the old tradition of weak-blonde-strong-brunette contrast is at work, stereotyping the fair Alice and dark Cora. These three are rather predictable types which both simplify and stultify the writer\\'s efforts with them. Magua\\'s stealthy eyes and abrupt, furtive actions mark him as a potential villain, while the exaggerated presentation of the simple, single-minded Gamut paints him as the comic and perhaps pitiable adult innocent. At this point, both are something less than realistic and fully vitalized characters, but in comparison to the other three they seem to breathe real air. The stature of originality and verisimilitude that they do show is doubtless due to the fact that they are native characters. One may note, for instance, that Heyward\\'s comment about Munro\\'s once dealing rigidly with Magua not only lends suspense to the situation and points to the theme of revenge but also suggests some depth of motivation for the Indian. What we call plot -- the complications of a situation and the subsequent events and actions that further entangle things before they are finally resolved in some fashion -- starts an early ferment in terms of danger and suspense. Four likable and somewhat innocent characters strike into the unknown forest wilderness with a doubtful guide. It is a time of urgency, and movement is swift. Cooper hardly gives the reader time to question seriously why Munro\\'s daughters would push forward their visit at this worst of times and would feel themselves safer almost alone on a dim path in savage-infested territory than in the company of fifteen hundred trained fighting men. This represents a lack in character motivation, but Cooper knows that he must get his people into jeopardy, and he at least partly succeeds in hiding this lack under suspenseful action and a sense of urgency. But in spite of the pace, Cooper also manages a good instance of dramatic irony, a fictional presentation in which the reader is allowed to see or deduce predicaments unknown or only partly known by the characters. It is thus that the first part of the pattern of action -- that of pursuit -- has begun.\"}',\n",
              "  'chapter': '\\n  \"Mine ear is open, and my heart prepared:\\n  The worst is worldly loss thou canst unfold:\\n  Say, is my kingdom lost?\"\\n\\n  SHAKESPEARE.\\n\\n\\nIt was a feature peculiar to the colonial wars of North America, that\\nthe toils and dangers of the wilderness were to be encountered before\\nthe adverse hosts could meet. A wide and apparently an impervious\\nboundary of forests severed the possessions of the hostile provinces of\\nFrance and England. The hardy colonist, and the trained European who\\nfought at his side, frequently expended months in struggling against the\\nrapids of the streams, or in effecting the rugged passes of the\\nmountains, in quest of an opportunity to exhibit their courage in a more\\nmartial conflict. But, emulating the patience and self-denial of the\\npractised native warriors, they learned to overcome every difficulty;\\nand it would seem that, in time, there was no recess of the woods so\\ndark, nor any secret place so lovely, that it might claim exemption from\\nthe inroads of those who had pledged their blood to satiate their\\nvengeance, or to uphold the cold and selfish policy of the distant\\nmonarchs of Europe.\\n\\nPerhaps no district throughout the wide extent of the intermediate\\nfrontiers can furnish a livelier picture of the cruelty and fierceness\\nof the savage warfare of those periods than the country which lies\\nbetween the head waters of the Hudson and the adjacent lakes.\\n\\nThe facilities which nature had there offered to the march of the\\ncombatants were too obvious to be neglected. The lengthened sheet of the\\nChamplain stretched from the frontiers of Canada, deep within the\\nborders of the neighboring province of New York, forming a natural\\npassage across half the distance that the French were compelled to\\nmaster in order to strike their enemies. Near its southern termination,\\nit received the contributions of another lake, whose waters were so\\nlimpid as to have been exclusively selected by the Jesuit missionaries\\nto perform the typical purification of baptism, and to obtain for it the\\ntitle of lake \"du Saint Sacrement.\" The less zealous English thought\\nthey conferred a sufficient honor on its unsullied fountains, when they\\nbestowed the name of their reigning prince, the second of the house of\\nHanover. The two united to rob the untutored possessors of its wooded\\nscenery of their native right to perpetuate its original appellation of\\n\"Horican.\"[1]\\n\\nWinding its way among countless islands, and imbedded in mountains, the\\n\"holy lake\" extended a dozen leagues still farther to the south. With\\nthe high plain that there interposed itself to the further passage of\\nthe water, commenced a portage of as many miles, which conducted the\\nadventurer to the banks of the Hudson, at a point where, with the usual\\nobstructions of the rapids, or rifts, as they were then termed in the\\nlanguage of the country, the river became navigable to the tide.\\n\\nWhile, in the pursuit of their daring plans of annoyance, the restless\\nenterprise of the French even attempted the distant and difficult gorges\\nof the Alleghany, it may easily be imagined that their proverbial\\nacuteness would not overlook the natural advantages of the district we\\nhave just described. It became, emphatically, the bloody arena, in which\\nmost of the battles for the mastery of the colonies were contested.\\nForts were erected at the different points that commanded the facilities\\nof the route, and were taken and retaken, razed and rebuilt, as victory\\nalighted on the hostile banners. While the husbandman shrank back from\\nthe dangerous passes, within the safer boundaries of the more ancient\\nsettlements, armies larger than those that had often disposed of the\\nsceptres of the mother countries, were seen to bury themselves in these\\nforests, whence they rarely returned but in skeleton bands, that were\\nhaggard with care, or dejected by defeat. Though the arts of peace were\\nunknown to this fatal region, its forests were alive with men; its\\nshades and glens rang with the sounds of martial music, and the echoes\\nof its mountains threw back the laugh, or repeated the wanton cry, of\\nmany a gallant and reckless youth, as he hurried by them, in the\\nnoontide of his spirits, to slumber in a long night of forgetfulness.\\n\\nIt was in this scene of strife and bloodshed that the incidents we shall\\nattempt to relate occurred, during the third year of the war which\\nEngland and France last waged for the possession of a country that\\nneither was destined to retain.\\n\\nThe imbecility of her military leaders abroad, and the fatal want of\\nenergy in her councils at home, had lowered the character of Great\\nBritain from the proud elevation on which it had been placed, by the\\ntalents and enterprise of her former warriors and statesmen. No longer\\ndreaded by her enemies, her servants were fast losing the confidence of\\nself-respect. In this mortifying abasement, the colonists, though\\ninnocent of her imbecility, and too humble to be the agents of her\\nblunders, were but the natural participators.\\n\\nThey had recently seen a chosen army from that country, which,\\nreverencing as a mother, they had blindly believed invincible--an army\\nled by a chief who had been selected from a crowd of trained warriors,\\nfor his rare military endowments, disgracefully routed by a handful of\\nFrench and Indians, and only saved from annihilation by the coolness and\\nspirit of a Virginian boy, whose riper fame has since diffused itself,\\nwith the steady influence of moral truth, to the uttermost confines of\\nChristendom.[2] A wide frontier had been laid naked by this unexpected\\ndisaster, and more substantial evils were preceded by a thousand\\nfanciful and imaginary dangers. The alarmed colonists believed that the\\nyells of the savages mingled with every fitful gust of wind that issued\\nfrom the interminable forests of the west. The terrific character of\\ntheir merciless enemies increased immeasurably the natural horrors of\\nwarfare. Numberless recent massacres were still vivid in their\\nrecollections; nor was there any ear in the provinces so deaf as not to\\nhave drunk in with avidity the narrative of some fearful tale of\\nmidnight murder, in which the natives of the forests were the principal\\nand barbarous actors. As the credulous and excited traveller related the\\nhazardous chances of the wilderness, the blood of the timid curdled\\nwith terror, and mothers cast anxious glances even at those children\\nwhich slumbered within the security of the largest towns. In short, the\\nmagnifying influence of fear began to set at naught the calculations of\\nreason, and to render those who should have remembered their manhood,\\nthe slaves of the basest of passions. Even the most confident and the\\nstoutest hearts began to think the issue of the contest was becoming\\ndoubtful; and that abject class was hourly increasing in numbers, who\\nthought they foresaw all the possessions of the English crown in America\\nsubdued by their Christian foes, or laid waste by the inroads of their\\nrelentless allies.\\n\\nWhen, therefore, intelligence was received at the fort, which covered\\nthe southern termination of the portage between the Hudson and the\\nlakes, that Montcalm had been seen moving up the Champlain, with an army\\n\"numerous as the leaves on the trees,\" its truth was admitted with more\\nof the craven reluctance of fear than with the stern joy that a warrior\\nshould feel, in finding an enemy within reach of his blow. The news had\\nbeen brought, towards the decline of a day in midsummer, by an Indian\\nrunner, who also bore an urgent request from Munro, the commander of a\\nwork on the shore of the \"holy lake,\" for a speedy and powerful\\nreinforcement. It has already been mentioned that the distance between\\nthese two posts was less than five leagues. The rude path, which\\noriginally formed their line of communication, had been widened for the\\npassage of wagons; so that the distance which had been travelled by the\\nson of the forest in two hours, might easily be effected by a detachment\\nof troops, with their necessary baggage, between the rising and setting\\nof a summer sun. The loyal servants of the British crown had given to\\none of these forest fastnesses the name of William Henry, and to the\\nother that of Fort Edward; calling each after a favorite prince of the\\nreigning family. The veteran Scotchman just named held the first, with a\\nregiment of regulars and a few provincials; a force really by far too\\nsmall to make head against the formidable power that Montcalm was\\nleading to the foot of his earthen mounds. At the latter, however, lay\\nGeneral Webb, who commanded the armies of the king in the northern\\nprovinces, with a body of more than five thousand men. By uniting the\\nseveral detachments of his command, this officer might have arrayed\\nnearly double that number of combatants against the enterprising\\nFrenchman, who had ventured so far from his reinforcements, with an army\\nbut little superior in numbers.\\n\\nBut under the influence of their degraded fortunes, both officers and\\nmen appeared better disposed to await the approach of their formidable\\nantagonists, within their works, than to resist the progress of their\\nmarch, by emulating the successful example of the French at Fort du\\nQuesne, and striking a blow on their advance.\\n\\nAfter the first surprise of the intelligence had a little abated, a\\nrumor was spread through the entrenched camp, which stretched along the\\nmargin of the Hudson, forming a chain of outworks to the body of the\\nfort itself, that a chosen detachment of fifteen hundred men was to\\ndepart, with the dawn, for William Henry, the post at the northern\\nextremity of the portage. That which at first was only rumor, soon\\nbecame certainty, as orders passed from the quarters of the\\ncommander-in-chief to the several corps he had selected for this\\nservice, to prepare for their speedy departure. All doubt as to the\\nintention of Webb now vanished, and an hour or two of hurried footsteps\\nand anxious faces succeeded. The novice in the military art flew from\\npoint to point, retarding his own preparations by the excess of his\\nviolent and somewhat distempered zeal; while the more practised veteran\\nmade his arrangements with a deliberation that scorned every appearance\\nof haste; though his sober lineaments and anxious eye sufficiently\\nbetrayed that he had no very strong professional relish for the as yet\\nuntried and dreaded warfare of the wilderness. At length the sun set in\\na flood of glory, behind the distant western hills, and as darkness drew\\nits veil around the secluded spot the sounds of preparation diminished;\\nthe last light finally disappeared from the log cabin of some officer;\\nthe trees cast their deeper shadows over the mounds and the rippling\\nstream, and a silence soon pervaded the camp, as deep as that which\\nreigned in the vast forest by which it was environed.\\n\\nAccording to the orders of the preceding night, the heavy sleep of the\\narmy was broken by the rolling of the warning drums, whose rattling\\nechoes were heard issuing, on the damp morning air, out of every vista\\nof the woods, just as day began to draw the shaggy outlines of some tall\\npines of the vicinity, on the opening brightness of a soft and cloudless\\neastern sky. In an instant the whole camp was in motion; the meanest\\nsoldier arousing from his lair to witness the departure of his\\ncomrades, and to share in the excitement and incidents of the hour. The\\nsimple array of the chosen band was soon completed. While the regular\\nand trained hirelings of the king marched with haughtiness to the right\\nof the line, the less pretending colonists took their humbler position\\non its left, with a docility that long practice had rendered easy. The\\nscouts departed; strong guards preceded and followed the lumbering\\nvehicles that bore the baggage; and before the gray light of the morning\\nwas mellowed by the rays of the sun, the main body of the combatants\\nwheeled into column, and left the encampment with a show of high\\nmilitary bearing, that served to drown the slumbering apprehensions of\\nmany a novice, who was now about to make his first essay in arms. While\\nin view of their admiring comrades, the same proud front and ordered\\narray was observed, until the notes of their fifes growing fainter in\\ndistance, the forest at length appeared to swallow up the living mass\\nwhich had slowly entered its bosom.\\n\\nThe deepest sounds of the retiring and invisible column had ceased to be\\nborne on the breeze to the listeners, and the latest straggler had\\nalready disappeared in pursuit; but there still remained the signs of\\nanother departure, before a log cabin of unusual size and\\naccommodations, in front of which those sentinels paced their rounds,\\nwho were known to guard the person of the English general. At this spot\\nwere gathered some half dozen horses, caparisoned in a manner which\\nshowed that two, at least, were destined to bear the persons of females,\\nof a rank that it was not usual to meet so far in the wilds of the\\ncountry. A third wore the trappings and arms of an officer of the staff;\\nwhile the rest, from the plainness of the housings, and the travelling\\nmails with which they were encumbered, were evidently fitted for the\\nreception of as many menials, who were, seemingly, already awaiting the\\npleasure of those they served. At a respectful distance from this\\nunusual show were gathered divers groups of curious idlers; some\\nadmiring the blood and bone of the high-mettled military charger, and\\nothers gazing at the preparations, with dull wonder of vulgar curiosity.\\nThere was one man, however, who, by his countenance and actions, formed\\na marked exception to those who composed the latter class of spectators,\\nbeing neither idle, nor seemingly very ignorant.\\n\\nThe person of this individual was to the last degree ungainly, without\\nbeing in any particular manner deformed. He had all the bones and joints\\nof other men, without any of their proportions. Erect, his stature\\nsurpassed that of his fellows; seated, he appeared reduced within the\\nordinary limits of the race. The same contrariety in his members seemed\\nto exist throughout the whole man. His head was large; his shoulders\\nnarrow; his arms long and dangling; while his hands were small, if not\\ndelicate. His legs and thighs were thin, nearly to emaciation, but of\\nextraordinary length; and his knees would have been considered\\ntremendous, had they not been outdone by the broader foundations on\\nwhich this false superstructure of the blended human orders was so\\nprofanely reared. The ill-assorted and injudicious attire of the\\nindividual only served to render his awkwardness more conspicuous. A\\nsky-blue coat, with short and broad skirts and low cape, exposed a long\\nthin neck, and longer and thinner legs, to the worst animadversions of\\nthe evil disposed. His nether garment was of yellow nankeen, closely\\nfitted to the shape, and tied at his bunches of knees by large knots of\\nwhite ribbon, a good deal sullied by use. Clouded cotton stockings, and\\nshoes, on one of the latter of which was a plated spur, completed the\\ncostume of the lower extremity of this figure, no curve or angle of\\nwhich was concealed, but, on the other hand, studiously exhibited,\\nthrough the vanity or simplicity of its owner. From beneath the flap of\\nan enormous pocket of a soiled vest of embossed silk, heavily ornamented\\nwith tarnished silver lace, projected an instrument, which, from being\\nseen in such martial company, might have been easily mistaken for some\\nmischievous and unknown implement of war. Small as it was, this uncommon\\nengine had excited the curiosity of most of the Europeans in the camp,\\nthough several of the provincials were seen to handle it, not only\\nwithout fear, but with the utmost familiarity. A large, civil cocked\\nhat, like those worn by clergymen within the last thirty years,\\nsurmounted the whole, furnishing dignity to a good-natured and somewhat\\nvacant countenance, that apparently needed such artificial aid, to\\nsupport the gravity of some high and extraordinary trust.\\n\\nWhile the common herd stood aloof, in deference to the quarters of Webb,\\nthe figure we have described stalked in the centre of the domestics,\\nfreely expressing his censures or commendations on the merits of the\\nhorses, as by chance they displeased or satisfied his judgment.\\n\\n\"This beast, I rather conclude, friend, is not of home raising, but is\\nfrom foreign lands, or perhaps from the little island itself over the\\nblue water?\" he said, in a voice as remarkable for the softness and\\nsweetness of its tones, as was his person for its rare proportions: \"I\\nmay speak of these things, and be no braggart; for I have been down at\\nboth havens; that which is situate at the mouth of Thames, and is named\\nafter the capital of Old England, and that which is called \\'Haven,\\' with\\nthe addition of the word \\'New\\'; and have seen the snows and brigantines\\ncollecting their droves, like the gathering to the ark, being outward\\nbound to the Island of Jamaica, for the purpose of barter and traffic in\\nfour-footed animals; but never before have I beheld a beast which\\nverified the true Scripture war-horse like this: \\'He paweth in the\\nvalley, and rejoiceth in his strength: he goeth on to meet the armed\\nmen. He saith among the trumpets, Ha, ha; and he smelleth the battle\\nafar off, the thunder of the captains, and the shouting.\\' It would seem\\nthat the stock of the horse of Israel has descended to our own time;\\nwould it not, friend?\"\\n\\nReceiving no reply to this extraordinary appeal, which in truth, as it\\nwas delivered with the vigor of full and sonorous tones, merited some\\nsort of notice, he who had thus sung forth the language of the Holy Book\\nturned to the silent figure to whom he had unwittingly addressed\\nhimself, and found a new and more powerful subject of admiration in the\\nobject that encountered his gaze. His eyes fell on the still, upright,\\nand rigid form of the \"Indian runner,\" who had borne to the camp the\\nunwelcome tidings of the preceding evening. Although in a state of\\nperfect repose, and apparently disregarding, with characteristic\\nstoicism, the excitement and bustle around him, there was a sullen\\nfierceness mingled with the quiet of the savage, that was likely to\\narrest the attention of much more experienced eyes than those which now\\nscanned him, in unconcealed amazement. The native bore both the tomahawk\\nand knife of his tribe; and yet his appearance was not altogether that\\nof a warrior. On the contrary, there was an air of neglect about his\\nperson, like that which might have proceeded from great and recent\\nexertion, which he had not yet found leisure to repair. The colors of\\nthe war-paint had blended in dark confusion about his fierce\\ncountenance, and rendered his swarthy lineaments still more savage and\\nrepulsive than if art had attempted an effect which had been thus\\nproduced by chance. His eye, alone, which glistened like a fiery star\\namid lowering clouds, was to be seen in its state of native wildness.\\nFor a single instant, his searching and yet wary glance met the\\nwondering look of the other, and then changing its direction, partly in\\ncunning, and partly in disdain, it remained fixed, as if penetrating the\\ndistant air.\\n\\nIt is impossible to say what unlooked-for remark this short and silent\\ncommunication, between two such singular men, might have elicited from\\nthe white man, had not his active curiosity been again drawn to other\\nobjects. A general movement among the domestics, and a low sound of\\ngentle voices, announced the approach of those whose presence alone was\\nwanted to enable the cavalcade to move. The simple admirer of the\\nwar-horse instantly fell back to a low, gaunt, switch-tailed mare, that\\nwas unconsciously gleaning the faded herbage of the camp nigh by; where,\\nleaning with one elbow on the blanket that concealed an apology for a\\nsaddle, he became a spectator of the departure, while a foal was quietly\\nmaking its morning repast, on the opposite side of the same animal.\\n\\nA young man, in the dress of an officer, conducted to their steeds two\\nfemales, who, as it was apparent by their dresses, were prepared to\\nencounter the fatigues of a journey in the woods. One, and she was the\\nmost juvenile in her appearance, though both were young, permitted\\nglimpses of her dazzling complexion, fair golden hair, and bright blue\\neyes, to be caught, as she artlessly suffered the morning air to blow\\naside the green veil which descended low from her beaver. The flush\\nwhich still lingered above the pines in the western sky was not more\\nbright nor delicate than the bloom on her cheek; nor was the opening day\\nmore cheering than the animated smile which she bestowed on the youth,\\nas he assisted her into the saddle. The other, who appeared to share\\nequally in the attentions of the young officer, concealed her charms\\nfrom the gaze of the soldiery, with a care that seemed better fitted to\\nthe experience of four or five additional years. It could be seen,\\nhowever, that her person, though moulded with the same exquisite\\nproportions, of which none of the graces were lost by the travelling\\ndress she wore, was rather fuller and more mature than that of her\\ncompanion.\\n\\nNo sooner were these females seated, than their attendant sprang lightly\\ninto the saddle of the war-horse, when the whole three bowed to Webb,\\nwho, in courtesy, awaited their parting on the threshold of his cabin,\\nand turning their horses\\' heads, they proceeded at a slow amble,\\nfollowed by their train, towards the northern entrance of the\\nencampment. As they traversed that short distance, not a voice was\\nheard amongst them; but a slight exclamation proceeded from the younger\\nof the females, as the Indian runner glided by her, unexpectedly, and\\nled the way along the military road in her front. Though this sudden and\\nstartling movement of the Indian produced no sound from the other, in\\nthe surprise her veil also was allowed to open its folds, and betrayed\\nan indescribable look of pity, admiration, and horror, as her dark eye\\nfollowed the easy motions of the savage. The tresses of this lady were\\nshining and black, like the plumage of the raven. Her complexion was not\\nbrown, but it rather appeared charged with the color of the rich blood,\\nthat seemed ready to burst its bounds. And yet there was neither\\ncoarseness nor want of shadowing in a countenance that was exquisitely\\nregular and dignified, and surpassingly beautiful. She smiled, as if in\\npity at her own momentary forgetfulness, discovering by the act a row of\\nteeth that would have shamed the purest ivory; when, replacing the veil,\\nshe bowed her face, and rode in silence, like one whose thoughts were\\nabstracted from the scene around her.\\n\\n\\n\\n\\n  \"Sola, sola, wo, ha, ho, sola!\"\\n\\n  SHAKESPEARE.\\n\\n\\nWhile one of the lovely beings we have so cursorily presented to the\\nreader was thus lost in thought, the other quickly recovered from the\\nalarm which induced the exclamation, and, laughing at her own weakness,\\nshe inquired of the youth who rode by her side,--\\n\\n\"Are such spectres frequent in the woods, Heyward; or is this sight an\\nespecial entertainment on our behalf? If the latter, gratitude must\\nclose our mouths; but if the former, both Cora and I shall have need to\\ndraw largely on that stock of hereditary courage which we boast, even\\nbefore we are made to encounter the redoubtable Montcalm.\"\\n\\n\"Yon Indian is a \\'runner\\' of the army; and, after the fashion of his\\npeople, he may be accounted a hero,\" returned the officer. \"He has\\nvolunteered to guide us to the lake, by a path but little known, sooner\\nthan if we followed the tardy movements of the column: and, by\\nconsequence, more agreeably.\"\\n\\n\"I like him not,\" said the lady, shuddering, partly in assumed, yet more\\nin real terror. \"You know him, Duncan, or you would not trust yourself\\nso freely to his keeping?\"\\n\\n\"Say, rather, Alice, that I would not trust you. I do know him, or he\\nwould not have my confidence, and least of all at this moment. He is\\nsaid to be a Canadian, too; and yet he served with our friends the\\nMohawks, who, as you know, are one of the six allied nations.[3] He was\\nbrought among us, as I have heard, by some strange accident in which\\nyour father was interested, and in which the savage was rigidly dealt\\nby--but I forget the idle tale; it is enough, that he is now our\\nfriend.\"\\n\\n\"If he has been my father\\'s enemy, I like him still less!\" exclaimed the\\nnow really anxious girl. \"Will you not speak to him, Major Heyward, that\\nI may hear his tones? Foolish though it may be, you have often heard me\\navow my faith in the tones of the human voice!\"\\n\\n\"It would be in vain; and answered, most probably, by an ejaculation.\\nThough he may understand it, he affects, like most of his people, to be\\nignorant of the English; and least of all will he condescend to speak\\nit, now that war demands the utmost exercise of his dignity. But he\\nstops; the private path by which we are to journey is, doubtless, at\\nhand.\"\\n\\nThe conjecture of Major Heyward was true. When they reached the spot\\nwhere the Indian stood, pointing into the thicket that fringed the\\nmilitary road, a narrow and blind path, which might, with some little\\ninconvenience, receive one person at a time, became visible.\\n\\n\"Here, then, lies our way,\" said the young man, in a low voice.\\n\"Manifest no distrust, or you may invite the danger you appear to\\napprehend.\"\\n\\n\"Cora, what think you?\" asked the reluctant fair one. \"If we journey\\nwith the troops, though we may find their presence irksome, shall we not\\nfeel better assurance of our safety?\"\\n\\n\"Being little accustomed to the practices of the savages, Alice, you\\nmistake the place of real danger,\" said Heyward. \"If enemies have\\nreached the portage at all, a thing by no means probable, as our scouts\\nare abroad, they will surely be found skirting the column where scalps\\nabound the most. The route of the detachment is known, while ours,\\nhaving been determined within the hour, must still be secret.\"\\n\\n\"Should we distrust the man because his manners are not our manners, and\\nthat his skin is dark?\" coldly asked Cora.\\n\\nAlice hesitated no longer; but giving her Narragansett[4] a smart cut\\nof the whip, she was the first to dash aside the slight branches of the\\nbushes, and to follow the runner along the dark and tangled pathway. The\\nyoung man regarded the last speaker in open admiration, and even\\npermitted her fairer though certainly not more beautiful companion to\\nproceed unattended, while he sedulously opened the way himself for the\\npassage of her who has been called Cora. It would seem that the\\ndomestics had been previously instructed; for, instead of penetrating\\nthe thicket, they followed the route of the column; a measure which\\nHeyward stated had been dictated by the sagacity of their guide, in\\norder to diminish the marks of their trail, if, haply, the Canadian\\nsavages should be lurking so far in advance of their army. For many\\nminutes the intricacy of the route admitted of no further dialogue;\\nafter which they emerged from the broad border of underbrush which grew\\nalong the line of the highway, and entered under the high but dark\\narches of the forest. Here their progress was less interrupted, and the\\ninstant the guide perceived that the females could command their steeds,\\nhe moved on, at a pace between a trot and a walk, and at a rate which\\nkept the sure-footed and peculiar animals they rode, at a fast yet easy\\namble. The youth had turned to speak to the dark-eyed Cora, when the\\ndistant sound of horses\\' hoofs, clattering over the roots of the broken\\nway in his rear, caused him to check his charger; and, as his companions\\ndrew their reins at the same instant, the whole party came to a halt, in\\norder to obtain an explanation of the unlooked-for interruption.\\n\\nIn a few moments a colt was seen gliding, like a fallow-deer, among the\\nstraight trunks of the pines; and, in another instant, the person of the\\nungainly man described in the preceding chapter, came into view, with as\\nmuch rapidity as he could excite his meagre beast to endure without\\ncoming to an open rupture. Until now this personage had escaped the\\nobservation of the travellers. If he possessed the power to arrest any\\nwandering eye when exhibiting the glories of his altitude on foot, his\\nequestrian graces were still more likely to attract attention.\\nNotwithstanding a constant application of his one armed heel to the\\nflanks of the mare, the most confirmed gait that he could establish was\\na Canterbury gallop with the hind legs, in which those more forward\\nassisted for doubtful moments, though generally content to maintain a\\nloping trot. Perhaps the rapidity of the changes from one of these paces\\nto the other created an optical illusion, which might thus magnify the\\npowers of the beast; for it is certain that Heyward, who possessed a\\ntrue eye for the merits of a horse, was unable, with his utmost\\ningenuity, to decide by what sort of movement his pursuer worked his\\nsinuous way on his footsteps with such persevering hardihood.\\n\\nThe industry and movements of the rider were not less remarkable than\\nthose of the ridden. At each change in the evolutions of the latter, the\\nformer raised his tall person in the stirrups; producing, in this\\nmanner, by the undue elongation of his legs, such sudden growths and\\ndiminishings of the stature, as baffled every conjecture that might be\\nmade as to his dimensions. If to this be added the fact that, in\\nconsequence of the ex parte application of the spur, one side of the\\nmare appeared to journey faster than the other; and that the aggrieved\\nflank was resolutely indicated by unremitted flourishes of a bushy tail,\\nwe finish the picture of both horse and man.\\n\\nThe frown which had gathered around the handsome, open, and manly brow\\nof Heyward, gradually relaxed, and his lips curled into a slight smile,\\nas he regarded the stranger. Alice made no very powerful effort to\\ncontrol her merriment; and even the dark, thoughtful eye of Cora lighted\\nwith a humor that, it would seem, the habit, rather than the nature of\\nits mistress repressed.\\n\\n\"Seek you any here?\" demanded Heyward, when the other had arrived\\nsufficiently nigh to abate his speed; \"I trust you are no messenger of\\nevil tidings?\"\\n\\n\"Even so,\" replied the stranger, making diligent use of his triangular\\ncastor, to produce a circulation in the close air of the woods, and\\nleaving his hearers in doubt to which of the young man\\'s questions he\\nresponded; when, however, he had cooled his face, and recovered his\\nbreath, he continued, \"I hear you are riding to William Henry; as I am\\njourneying thitherward myself, I concluded good company would seem\\nconsistent to the wishes of both parties.\"\\n\\n\"You appear to possess the privilege of a casting vote,\" returned\\nHeyward; \"we are three, whilst you have consulted no one but yourself.\"\\n\\n\"Even so. The first point to be obtained is to know one\\'s own mind. Once\\nsure of that, and where women are concerned, it is not easy, the next\\nis, to act up to the decision. I have endeavored to do both, and here I\\nam.\"\\n\\n\"If you journey to the lake, you have mistaken your route,\" said\\nHeyward, haughtily; \"the highway thither is at least half a mile behind\\nyou.\"\\n\\n\"Even so,\" returned the stranger, nothing daunted by this cold\\nreception; \"I have tarried at \\'Edward\\' a week, and I should be dumb not\\nto have inquired the road I was to journey; and if dumb there would be\\nan end to my calling.\" After simpering in a small way, like one whose\\nmodesty prohibited a more open expression of his admiration of a\\nwitticism that was perfectly unintelligible to his hearers, he\\ncontinued: \"It is not prudent for any one of my profession to be too\\nfamiliar with those he is to instruct; for which reason I follow not the\\nline of the army; besides which, I conclude that a gentleman of your\\ncharacter has the best judgment in matters of wayfaring; I have\\ntherefore decided to join company, in order that the ride may be made\\nagreeable, and partake of social communion.\"\\n\\n\"A most arbitrary, if not a hasty decision!\" exclaimed Heyward,\\nundecided whether to give vent to his growing anger, or to laugh in the\\nother\\'s face. \"But you speak of instruction, and of a profession; are\\nyou an adjunct to the provincial corps, as a master of the noble science\\nof defence and offence; or, perhaps, you are one who draws lines and\\nangles, under the pretence of expounding the mathematics?\"\\n\\nThe stranger regarded his interrogator a moment, in wonder; and then,\\nlosing every mark of self-satisfaction in an expression of solemn\\nhumility, he answered:--\\n\\n\"Of offence, I hope there is none, to either party: of defence, I make\\nnone--by God\\'s good mercy, having committed no palpable sin since last\\nentreating his pardoning grace. I understand not your allusions about\\nlines and angles; and I leave expounding to those who have been called\\nand set apart for that holy office. I lay claim to no higher gift than a\\nsmall insight into the glorious art of petitioning and thanksgiving, as\\npractised in psalmody.\"\\n\\n\"The man is, most manifestly, a disciple of Apollo,\" cried the amused\\nAlice, \"and I take him under my own especial protection. Nay, throw\\naside that frown, Heyward, and in pity to my longing ears, suffer him to\\njourney in our train. Besides,\" she added, in a low and hurried voice,\\ncasting a glance at the distant Cora, who slowly followed the footsteps\\nof their silent but sullen guide, \"it may be a friend added to our\\nstrength, in time of need.\"\\n\\n\"Think you, Alice, that I would trust those I love by this secret path,\\ndid I imagine such need could happen?\"\\n\\n\"Nay, nay, I think not of it now; but this strange man amuses me; and if\\nhe \\'hath music in his soul,\\' let us not churlishly reject his company.\"\\nShe pointed persuasively along the path with her riding-whip, while\\ntheir eyes met in a look which the young man lingered a moment to\\nprolong; then yielding to her gentle influence, he clapped his spurs\\ninto his charger, and in a few bounds was again at the side of Cora.\\n\\n\"I am glad to encounter thee, friend,\" continued the maiden, waving her\\nhand to the stranger to proceed, as she urged her Narragansett to renew\\nits amble. \"Partial relatives have almost persuaded me that I am not\\nentirely worthless in a duet myself; and we may enliven our wayfaring by\\nindulging in our favorite pursuit. It might be of signal advantage to\\none, ignorant as I, to hear the opinions and experience of a master in\\nthe art.\"\\n\\n\"It is refreshing both to the spirits and to the body to indulge in\\npsalmody, in befitting seasons,\" returned the master of song,\\nunhesitatingly complying with her intimation to follow; \"and nothing\\nwould relieve the mind more than such a consoling communion. But four\\nparts are altogether necessary to the perfection of melody. You have all\\nthe manifestations of a soft and rich treble; I can, by especial aid,\\ncarry a full tenor to the highest letter; but we lack counter and bass!\\nYon officer of the king, who hesitated to admit me to his company, might\\nfill the latter, if one may judge from the intonations of his voice in\\ncommon dialogue.\"\\n\\n\"Judge not too rashly from hasty and deceptive appearances,\" said the\\nlady, smiling; \"though Major Heyward can assume such deep notes on\\noccasion, believe me, his natural tones are better fitted for a mellow\\ntenor than the bass you heard.\"\\n\\n\"Is he, then, much practised in the art of psalmody?\" demanded her\\nsimple companion.\\n\\nAlice felt disposed to laugh, though she succeeded in suppressing her\\nmerriment, ere she answered,--\\n\\n\"I apprehend that he is rather addicted to profane song. The chances of\\na soldier\\'s life are but little fitted for the encouragement of more\\nsober inclinations.\"\\n\\n\"Man\\'s voice is given to him, like his other talents, to be used, and\\nnot to be abused. None can say they have ever known me neglect my gifts!\\nI am thankful that, though my boyhood may be said to have been set\\napart, like the youth of the royal David, for the purposes of music, no\\nsyllable of rude verse has ever profaned my lips.\"\\n\\n\"You have, then, limited your efforts to sacred song?\"\\n\\n\"Even so. As the psalms of David exceed all other language, so does the\\npsalmody that has been fitted to them by the divines and sages of the\\nland, surpass all vain poetry. Happily, I may say that I utter nothing\\nbut the thoughts and the wishes of the King of Israel himself; for\\nthough the times may call for some slight changes, yet does this version\\nwhich we use in the colonies of New England, so much exceed all other\\nversions, that, by its richness, its exactness, and its spiritual\\nsimplicity, it approacheth, as near as may be, to the great work of the\\ninspired writer. I never abide in any place, sleeping or waking, without\\nan example of this gifted work. \\'Tis the six-and-twentieth edition,\\npromulgated at Boston, Anno Domini 1744; and is entitled, _The Psalms,\\nHymns, and Spiritual Songs of the Old and New Testaments; faithfully\\ntranslated into English Metre, for the Use, Edification, and Comfort of\\nthe Saints, in Public and Private, especially in New England_.\"\\n\\nDuring this eulogium on the rare production of his native poets, the\\nstranger had drawn the book from his pocket, and, fitting a pair of\\niron-rimmed spectacles to his nose, opened the volume with a care and\\nveneration suited to its sacred purposes. Then, without circumlocution\\nor apology, first pronouncing the word \"Standish,\" and placing the\\nunknown engine, already described, to his mouth, from which he drew a\\nhigh, shrill sound, that was followed by an octave below, from his own\\nvoice, he commenced singing the following words, in full, sweet, and\\nmelodious tones, that set the music, the poetry, and even the uneasy\\nmotion of his ill-trained beast at defiance:--\\n\\n  \"How good it is, O see,\\n    And how it pleaseth well,\\n  Together, e\\'en in unity,\\n    For brethren so to dwell.\\n  It\\'s like the choice ointment,\\n    From the head to the beard did go:\\n  Down Aaron\\'s beard, that downward went,\\n    His garment\\'s skirts unto.\"\\n\\nThe delivery of these skilful rhymes was accompanied, on the part of the\\nstranger, by a regular rise and fall of his right hand, which\\nterminated at the descent, by suffering the fingers to dwell a moment on\\nthe leaves of the little volume; and on the ascent, by such a flourish\\nof the member as none but the initiated may ever hope to imitate. It\\nwould seem that long practice had rendered this manual accompaniment\\nnecessary; for it did not cease until the preposition which the poet had\\nselected for the close of his verse, had been duly delivered like a word\\nof two syllables.\\n\\nSuch an innovation on the silence and retirement of the forest could not\\nfail to enlist the ears of those who journeyed at so short a distance in\\nadvance. The Indian muttered a few words in broken English to Heyward,\\nwho, in his turn, spoke to the stranger; at once interrupting, and, for\\nthe time, closing his musical efforts.\\n\\n\"Though we are not in danger, common prudence would teach us to journey\\nthrough this wilderness in as quiet a manner as possible. You will,\\nthen, pardon me, Alice, should I diminish your enjoyments, by requesting\\nthis gentleman to postpone his chant until a safer opportunity.\"\\n\\n\"You will diminish them, indeed,\" returned the arch girl, \"for never did\\nI hear a more unworthy conjunction of execution and language, than that\\nto which I have been listening; and I was far gone in a learned inquiry\\ninto the causes of such an unfitness between sound and sense, when you\\nbroke the charm of my musings by that bass of yours, Duncan!\"\\n\\n\"I know not what you call my bass,\" said Heyward, piqued at her remark,\\n\"but I know that your safety, and that of Cora, is far dearer to me than\\ncould be any orchestra of Handel\\'s music.\" He paused and turned his head\\nquickly towards a thicket, and then bent his eyes suspiciously on their\\nguide, who continued his steady pace, in undisturbed gravity. The young\\nman smiled to himself, for he believed he had mistaken some shining\\nberry of the woods for the glistening eyeballs of a prowling savage, and\\nhe rode forward, continuing the conversation which had been interrupted\\nby the passing thought.\\n\\nMajor Heyward was mistaken only in suffering his youthful and generous\\npride to suppress his active watchfulness. The cavalcade had not long\\npassed, before the branches of the bushes that formed the thicket were\\ncautiously moved asunder, and a human visage, as fiercely wild as savage\\nart and unbridled passions could make it, peered out on the retiring\\nfootsteps of the travellers. A gleam of exultation shot across the\\ndarkly painted lineaments of the inhabitant of the forest, as he traced\\nthe route of his intended victims, who rode unconsciously onward; the\\nlight and graceful forms of the females waving among the trees, in the\\ncurvatures of their path, followed at each bend by the manly figure of\\nHeyward, until, finally, the shapeless person of the singing-master was\\nconcealed behind the numberless trunks of trees, that rose, in dark\\nlines, in the intermediate space.\\n\\n\\n\\n',\n",
              "  'chapter_length': 6471.0,\n",
              "  'summary_name': 'Chapters 1-2',\n",
              "  'summary_url': 'https://web.archive.org/web/20201101053205/https://www.cliffsnotes.com/literature/l/the-last-of-the-mohicans/summary-and-analysis/chapters-12',\n",
              "  'summary_text': 'Before any characters appear, the time and geography are made clear. Though it is the last war that England and France waged for a country that neither would retain, the wilderness between the forces still has to be overcome first. Thus it is in 1757, in the New York area between the head waters of the Hudson River and Lake George to the north. Because only two years earlier General Braddock was disgracefully routed by a handful of French and Indians, the frontier is now exposed to real and imaginary savage disasters as well as to the horrors of warfare. Fear has replaced reason. Near dusk of a day in July, an Indian runner named Magua arrives at Fort Edward on the upper Hudson. He has come from Fort William Henry at the southern tip of Lake George with the news that the French General Montcalm is moving south with a very large army and that Munro, commander of Fort William Henry, is in urgent need of plentiful reinforcements from General Webb. Early the next morning, a limited detachment of fifteen hundred regulars and colonists departs as if swallowed by the forest. Shortly afterwards, Major Duncan Heyward and Alice and Cora Munro, guided by Magua on foot, take by horseback a secret route toward William Henry for the girls to join their father. Blonde Alice is doubtful about Magua, covered with war paint and showing a sullen fierceness; but dark-haired Cora is stoically common sense about him, even though Heyward mentions that their father had once had to deal rigidly with the Indian. As the small party pushes on, they are overtaken by David Gamut, a tall, ungainly psalmodist ridiculously dressed and carrying a pitch pipe while riding a mare followed by its young colt. He desires to join them, and after some banter between him and Alice, he pulls out the twenty-sixth edition of The Bay Psalm Book, sounds his pipe, and renders a song \"in full, sweet, and melodious tones.\" At a muttered comment from Magua, Heyward insists upon silence for safety. Then he glances about them and, satisfied that he has seen only shining berries, smiles to himself as they move on. But he is wrong. The branches move and a man peers exultingly after them as they disappear among the dark lines of trees.',\n",
              "  'summary_analysis': \"These two chapters introduce the reader to the historical and natural settings and are indicative of the extent to which this book, as a historical novel, relates its fictional characters to real history. Only here at the beginning and later at mid-novel will the action coincide in detail with actual events, though the historic war is always somewhere in the near distance. These chapters also present four of the main fictional characters and one secondary one, all of whom will merit our concern henceforth. Major Heyward is the gallant romantic hero, but unlike most sentimental romances where for each hero there is one heroine, here there are two, Alice and Cora, blonde and brunette. And it is immediately apparent that the old tradition of weak-blonde-strong-brunette contrast is at work, stereotyping the fair Alice and dark Cora. These three are rather predictable types which both simplify and stultify the writer's efforts with them. Magua's stealthy eyes and abrupt, furtive actions mark him as a potential villain, while the exaggerated presentation of the simple, single-minded Gamut paints him as the comic and perhaps pitiable adult innocent. At this point, both are something less than realistic and fully vitalized characters, but in comparison to the other three they seem to breathe real air. The stature of originality and verisimilitude that they do show is doubtless due to the fact that they are native characters. One may note, for instance, that Heyward's comment about Munro's once dealing rigidly with Magua not only lends suspense to the situation and points to the theme of revenge but also suggests some depth of motivation for the Indian. What we call plot -- the complications of a situation and the subsequent events and actions that further entangle things before they are finally resolved in some fashion -- starts an early ferment in terms of danger and suspense. Four likable and somewhat innocent characters strike into the unknown forest wilderness with a doubtful guide. It is a time of urgency, and movement is swift. Cooper hardly gives the reader time to question seriously why Munro's daughters would push forward their visit at this worst of times and would feel themselves safer almost alone on a dim path in savage-infested territory than in the company of fifteen hundred trained fighting men. This represents a lack in character motivation, but Cooper knows that he must get his people into jeopardy, and he at least partly succeeds in hiding this lack under suspenseful action and a sense of urgency. But in spite of the pace, Cooper also manages a good instance of dramatic irony, a fictional presentation in which the reader is allowed to see or deduce predicaments unknown or only partly known by the characters. It is thus that the first part of the pattern of action -- that of pursuit -- has begun.\",\n",
              "  'summary_length': 388.0,\n",
              "  'analysis_length': 473.0},\n",
              " 'truncated_cells': []}"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "book_dataset['rows'][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jOO5MVj7Xq24",
        "outputId": "8381701e-aedf-479c-f75c-650ea9b3591f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "40844"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "len(book_dataset['rows'][0]['row']['chapter'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQVxnjAaXq24"
      },
      "source": [
        "As you can see the chapters are pretty long. Let's see how many tokens we have in those chapters. Based on instruction from OpenAI we need a package `tiktoken`\n",
        "\n",
        "We'll be following this instruction:\n",
        "\n",
        "https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0LMi73TXXq24",
        "outputId": "7a974d8b-1796-446c-e274-e8663053348c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (0.5.2)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2023.6.3)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2023.11.17)\n"
          ]
        }
      ],
      "source": [
        "!pip install tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OnZO00sCXq24"
      },
      "outputs": [],
      "source": [
        "import tiktoken\n",
        "encoder = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w-1X4FszXq24",
        "outputId": "9f951a3f-9016-4624-c158-64b05ee2de89",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[9906, 4435, 0]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "encoder.encode(\"Hello World!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Please write a small function to count tokens:"
      ],
      "metadata": {
        "id": "wcb_hZAsbrnL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UH2U-o6DXq25"
      },
      "outputs": [],
      "source": [
        "def count_chatgpt_tokens(text: str, tokenizer: tiktoken.Encoding) -> int:\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SOjc2wBXXq25"
      },
      "outputs": [],
      "source": [
        "def count_chatgpt_tokens(text: str, tokenizer: tiktoken.Encoding) -> int:\n",
        "    return len(encoder.encode(text))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's check how many tokens are there in a very simple string:"
      ],
      "metadata": {
        "id": "GyojR5LtbvQc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eCQwEhCLXq25",
        "outputId": "c2ee068b-61e4-492c-d48b-337b511457d9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "count_chatgpt_tokens(\"Hello world!\", tokenizer=encoder)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ESlu-SQXq25"
      },
      "source": [
        "Now that we have this function, let's fing the maximum token length of a chapter?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zeKeXWepXq25",
        "outputId": "09c0ae36-5108-4c7e-a6f2-9866e29657cf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "13237"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "max(\n",
        "    count_chatgpt_tokens(row['row']['chapter'], encoder)\n",
        "    for row in\n",
        "    book_dataset['rows']\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B2fLupZBXq25"
      },
      "source": [
        "As we can see at the api reference page, `gpt-3.5-turbo ` has only 4096 tokens context length. Our chapters are longer then that. We could still use `pt-3.5-turbo-16k`, but it's easy to imagine texts that are longer than this, so let's learn how to do it with a smaller-context LLM.\n",
        "\n",
        "An obvious way to cope with the problem is to:\n",
        "1. Split the text into chunks of sentences that can fit into the context window.\n",
        "2. Summarize each of the chunks.\n",
        "3. Concatenate all the summaries. If the total length is still too big, repeat the steps 1 and 2 until it's ok.\n",
        "4. Summarize the concatenations of the summaries.\n",
        "\n",
        "## Your task\n",
        "\n",
        "Write a function\n",
        "\n",
        "```summarize_long_text_with_chatgpt(chapter: str) -> str```\n",
        "\n",
        "implementing the above method of summarization.\n",
        "\n",
        "Don't forget to log the lengths each iteration to see how much texts shrink.\n",
        "\n",
        "For a given example please analyse your intermediate and final results. Is it indeed a good summary of the text?\n",
        "\n",
        "**Hints and suggestions**:\n",
        "- Keep in mind, that MAX TOKENS restrictions takes into account both request and model answer. So, you also need to leave some tokens for a response. So we'd suggest using at least 2:1 token ratio for chapter and summary.\n",
        "- You can control the length of the summary with prompts.\n",
        "- If you just use `split(\".\")`, you won't get a proper splitting into sentences. Luckily we have convenient Python libraries for text processing. We recommend using `sent_tokenize` or `split_into_sentences` from the `nltk` library. You can also try splitting the text into chunks of paragraphs instead.\n",
        "- It's difficult to measure the quality of summarization, but please analyze at lease two examples. Are the summaries coherent?\n",
        "- If you need inspiration in prompt building, take a look at this [paper](https://arxiv.org/pdf/2312.16171v1.pdf)\n",
        "\n",
        "**Bonus parts:**\n",
        "\n",
        "- Summarized text often starts with something like \"This text is about\", and after merging the partial summaries you'll probably have things like that all over the text. You may wish to get rid of such introductory phrases either by tuning a prompt or by post editing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S0Jzyc3kXq25",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02b4b6b8-6ac3-43f5-9982-766db54b4375"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai==0.28\n",
            "  Using cached openai-0.28.0-py3-none-any.whl (76 kB)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (4.66.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (3.9.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (2023.11.17)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (23.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.9.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.4.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (4.0.3)\n",
            "Installing collected packages: openai\n",
            "  Attempting uninstall: openai\n",
            "    Found existing installation: openai 1.7.2\n",
            "    Uninstalling openai-1.7.2:\n",
            "      Successfully uninstalled openai-1.7.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed openai-0.28.0\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install openai==0.28\n",
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rOVlg2hAXq26"
      },
      "outputs": [],
      "source": [
        "def summarize_long_text_with_chatgpt(chapter: str) -> str:\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "import openai\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "openai.api_key = open(\"/content/drive/MyDrive/.open-ai-api-key.txt\").read().strip()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xPg4_FBKLbM5",
        "outputId": "17d279d5-d7ec-4129-8d72-aad4b5df68c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "import logging\n",
        "\n",
        "def summarize_long_text_with_chatgpt(chapter: str) -> str:\n",
        "    MAX_TOKENS = 4096  # Max tokens for GPT-3.5-turbo\n",
        "    SUMMARY_RATIO = 2  # Ratio of chapter to summary length\n",
        "\n",
        "    def summarize(text: str) -> str:\n",
        "        # Adjust the prompt to control the summary length\n",
        "        prompt = f\"Summarize the following text:\\n\\n{text}\"\n",
        "        response = openai.ChatCompletion.create(\n",
        "            model=\"gpt-3.5-turbo\",\n",
        "            messages=[{\"role\": \"system\", \"content\": \"You are a summarization assistant.\"},\n",
        "                      {\"role\": \"user\", \"content\": prompt}]\n",
        "        )\n",
        "        return response.choices[0].message['content'].strip()\n",
        "\n",
        "    def split_text(text: str, max_length: int) -> list:\n",
        "        sentences = sent_tokenize(text)\n",
        "        chunks = []\n",
        "        current_chunk = \"\"\n",
        "\n",
        "        for sentence in sentences:\n",
        "            if len(current_chunk) + len(sentence) < max_length:\n",
        "                current_chunk += sentence + \" \"\n",
        "            else:\n",
        "                chunks.append(current_chunk)\n",
        "                current_chunk = sentence + \" \"\n",
        "        chunks.append(current_chunk)  # Add the last chunk\n",
        "        return chunks\n",
        "\n",
        "    logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "    # Initial split and summarize\n",
        "    chunks = split_text(chapter, MAX_TOKENS)\n",
        "    summaries = [summarize(chunk) for chunk in chunks]\n",
        "    combined_summary = \" \".join(summaries)\n",
        "\n",
        "    # Iteratively summarize if the text is still too long\n",
        "    iteration = 1\n",
        "    while len(combined_summary.split()) > MAX_TOKENS / SUMMARY_RATIO:\n",
        "        logging.info(f\"Iteration {iteration}, length: {len(combined_summary.split())}\")\n",
        "        chunks = split_text(combined_summary, MAX_TOKENS)\n",
        "        summaries = [summarize(chunk) for chunk in chunks]\n",
        "        combined_summary = \" \".join(summaries)\n",
        "        iteration += 1\n",
        "\n",
        "    logging.info(f\"Final iteration {iteration}, length: {len(combined_summary.split())}\")\n",
        "    return combined_summary\n"
      ],
      "metadata": {
        "id": "u-qYPQRKLPTU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-mPvXrkwXq26",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba4ebd61-ac9c-421f-e827-9def773120d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "During the colonial wars in North America, the toils and dangers of the wilderness had to be overcome before the opposing forces could engage in conflict. The boundaries between the French and English\n",
            "7270\n"
          ]
        }
      ],
      "source": [
        "sample_chapter = book_dataset['rows'][0]['row']['chapter']\n",
        "summarized_chapter = summarize_long_text_with_chatgpt(sample_chapter)\n",
        "print(summarized_chapter[:200])\n",
        "print(len(summarized_chapter))\n",
        "assert len(summarized_chapter) < len(sample_chapter) // 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iLa5rxx4Xq26"
      },
      "source": [
        "# Task 2. Extracting information with LLMs\n",
        "\n",
        "4 points\n",
        "\n",
        "At the practice session we were usually happy if we got something coherent. However, in real applications we often need to obtain concrete answers. Let's explore how to do it with LLMs."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's imagine that you work for a marketing agency, and you need to gather analytics about the passing events dedicated to AI and Machine Learning. For that, you need to process press releases and extract:\n",
        "- Event name,\n",
        "- Event date,\n",
        "- Number of participants,\n",
        "- Number of speakers,\n",
        "- Attendance price.\n",
        "\n",
        "Of course, you can do it manually, but it's much more fun to use Generative AI! So, your task will be to write a function that does this with only one request to OpenAI API.\n",
        "\n",
        "Below there is an example of a press release (generated by ChatGPT, of course, so that both the event and the personae are fictional). All of them are in the press_releases.zip archive in the hometask week 1 folder.\n",
        "\n",
        "<blockquote>\n",
        "<p>PRESS RELEASE\n",
        "\n",
        "InnovAI Summit 2023: A Glimpse into the Future of Artificial Intelligence</p>\n",
        "\n",
        "City of Virtue, Cyberspace - November 8, 2023 - The most anticipated event of the year, InnovAI Summit 2023, successfully concluded last weekend, on November 5, 2023. Held in the state-of-the-art VirtuTech Arena, the summit saw a massive turnout of over 3,500 participants, from brilliant AI enthusiasts and researchers to pioneers in the field.\n",
        "\n",
        "Esteemed speakers took to the stage to shed light on the latest breakthroughs, practical implementations, and ethical considerations in AI. Dr. Evelyn Quantum, renowned for her groundbreaking work on Quantum Machine Learning, emphasized the importance of this merger and how it's revolutionizing computing as we know it. Another keynote came from Prof. Leo Nexus, whose current project 'AI for Sustainability' highlights the symbiotic relationship between nature and machine, aiming to use AI in restoring our planet's ecosystems.\n",
        "\n",
        "This year's panel discussion, moderated by the talented Dr. Ada Neura, featured lively debates on the limits of AI in creative arts. Renowned digital artist, Felix Vortex, showcased how he uses generative adversarial networks to create surreal art pieces, while bestselling author, Iris Loom, explained her experiments with AI-assisted story crafting.\n",
        "\n",
        "Among other highlights were hands-on workshops, interactive Q&A sessions, and an 'AI & Ethics' debate which was particularly well-received, emphasizing the need for transparency and fairness in AI models. An exclusive 'Start-up Alley' allowed budding entrepreneurs to showcase their innovations, gaining attention from global venture capitalists and media.\n",
        "\n",
        "The event wrapped up with an announcement for InnovAI Summit 2024, set to be even grander. Participants left with a renewed enthusiasm for the vast possibilities that the AI and ML world promises.\n",
        "\n",
        "For media inquiries, please contact:\n",
        "Jane Cipher\n",
        "Director of Communications, InnovAI Summit\n",
        "Email: jane.cipher@innovai.org\n",
        "Phone: +123-4567-8910</p>\n",
        "</blockquote>\n",
        "\n",
        "More specifically, you should write a function\n",
        "\n",
        "```python\n",
        "parse_press_release(pr: str) -> dict\n",
        "```\n",
        "\n",
        "where the output should be in the format\n",
        "\n",
        "```python\n",
        "{\n",
        "  name: 'InnovAI Summit 2023',\n",
        "  date: '08.11.2023',\n",
        "  n_participants: 3500,\n",
        "  n_speakers: 4,\n",
        "  price:\n",
        "}\n",
        "```\n",
        "\n",
        "If any of the four characteristics is not mentioned in the text, put `None` in the respective field.\n",
        "\n",
        "At the end, calculate the statistics of right answers and analyse what kind of mistakes you \"model\" makes the most."
      ],
      "metadata": {
        "id": "SI_2o1xy_PeY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Hints and suggestions:**\n",
        "- It's gonna be more convenient to experiment in OpenAI chat interface https://chat.openai.com/. Plus this doesn't cost API requests money.\n",
        "- You need to be very accurate with what you want from the model.\n",
        "- It will help if you specify in the prompt that the output should be in JSON format, this way you will spend less time parsing the output.\n",
        "- Please be careful with the details. For example, Jane Cipher in the text above is not a speaker and shouldn't be counter as such (how to get rid of a contact person?). Also pay attention to the date format,\n",
        "- If the model is too wilful with the output format, don't hesitate to show some examples. Decreasing the temperature of predictions can help reduce the creativity of the answer, which is what we want for such task.\n",
        "- Debugging an LLM-powered application may become a tough business. When you think that you've polished it, an LLM can still surprise you. So, we don't expect 100% accuracy in this task, but we expect that you do your best to achieve high quality results."
      ],
      "metadata": {
        "id": "RkEWdffj_Phj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "press_release = \"\"\"PRESS RELEASE\n",
        "\n",
        "InnovAI Summit 2023: A Glimpse into the Future of Artificial Intelligence\n",
        "\n",
        "City of Virtue, Cyberspace - November 8, 2023 - The most anticipated event of the year, InnovAI Summit 2023, successfully concluded last weekend, on November 5, 2023. Held in the state-of-the-art VirtuTech Arena, the summit saw a massive turnout of over 3,500 participants, from brilliant AI enthusiasts and researchers to pioneers in the field.\n",
        "\n",
        "Esteemed speakers took to the stage to shed light on the latest breakthroughs, practical implementations, and ethical considerations in AI. Dr. Evelyn Quantum, renowned for her groundbreaking work on Quantum Machine Learning, emphasized the importance of this merger and how it's revolutionizing computing as we know it. Another keynote came from Prof. Leo Nexus, whose current project 'AI for Sustainability' highlights the symbiotic relationship between nature and machine, aiming to use AI in restoring our planet's ecosystems.\n",
        "\n",
        "This year's panel discussion, moderated by the talented Dr. Ada Neura, featured lively debates on the limits of AI in creative arts. Renowned digital artist, Felix Vortex, showcased how he uses generative adversarial networks to create surreal art pieces, while bestselling author, Iris Loom, explained her experiments with AI-assisted story crafting.\n",
        "\n",
        "Among other highlights were hands-on workshops, interactive Q&A sessions, and an 'AI & Ethics' debate which was particularly well-received, emphasizing the need for transparency and fairness in AI models. An exclusive 'Start-up Alley' allowed budding entrepreneurs to showcase their innovations, gaining attention from global venture capitalists and media.\n",
        "\n",
        "The event wrapped up with an announcement for InnovAI Summit 2024, set to be even grander. Participants left with a renewed enthusiasm for the vast possibilities that the AI and ML world promises.\n",
        "\n",
        "For media inquiries, please contact: Jane Cipher Director of Communications, InnovAI Summit Email: jane.cipher@innovai.org Phone: +123-4567-8910\"\"\""
      ],
      "metadata": {
        "id": "kdpHnyhxB_t3"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai==0.28\n",
        "import openai\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "openai.api_key = open(\"/content/drive/MyDrive/.open-ai-api-key.txt\").read().strip()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tRmqrVeG5UWY",
        "outputId": "ff4b2b66-f666-41f8-909c-18a373fe9468"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai==0.28\n",
            "  Downloading openai-0.28.0-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m76.5/76.5 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (4.66.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (3.9.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (2024.2.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (4.0.3)\n",
            "Installing collected packages: openai\n",
            "  Attempting uninstall: openai\n",
            "    Found existing installation: openai 1.11.1\n",
            "    Uninstalling openai-1.11.1:\n",
            "      Successfully uninstalled openai-1.11.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires tiktoken, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed openai-0.28.0\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_press_release(pr: str) -> dict:\n",
        "    pass"
      ],
      "metadata": {
        "id": "9JElaQayBfvl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "def parse_press_release(pr: str) -> dict:\n",
        "    prompt = (\n",
        "        f\"Here's a press release\\n{pr}\\n\\nExtract from it the following json:\"\\\n",
        "       '{\"name\": NAME_OF_EVENT, \"date\": DATE_OF_EVENT, \"n_participants\": NUM_PARTICIPANTS, \"n_speakers\": NUM_SPEAKERS, \"price\": PRICE}'\\\n",
        "       \"NAME_OF_EVENT should be the name of event advertised,\\n\"\\\n",
        "       \"DATE_OF_EVENT should be the date of event mentioned in format DD.MM.YYYY or DD.MM.YYYY-DD.MM.YYYY if the event lasted for several days,\\n\"\\\n",
        "       \"NUM_PARTICIPANTS should be the estimated amount of participants of said event in a format like 200 or 1000 or 10000, do not write it like 2,000,\\n\"\\\n",
        "       \"NUM_SPEAKERS is a number, corresponding to amount of names of speakers and hosts mentioned\\n\"\\\n",
        "       \"PRICE should be the price of event in the format EUR 100 or USD 1000 or GBP 100 depending on currency. Do not write currency symbol, instead write an abbreviation.\\n\"\\\n",
        "       \"If any information needed for JSON is not available, write 'No information mentioned' instead.\"\n",
        "    )\n",
        "\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a data extraction assistant.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    # Extract and parse the response\n",
        "    try:\n",
        "        extracted_info = response.choices[0].message['content'].strip()\n",
        "        # Assuming the model returns a JSON-like string\n",
        "        info_dict = json.loads(extracted_info)\n",
        "        return info_dict\n",
        "    except Exception as e:\n",
        "        print(f\"Error in parsing response: {e}\")\n",
        "        return {}\n"
      ],
      "metadata": {
        "id": "edce_tRhjdB0"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parse_press_release(press_release)"
      ],
      "metadata": {
        "id": "DhbK6gNtDK7w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3f01da1-b1f1-4ee9-84ac-d9a9e9dc09ad"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'name': 'InnovAI Summit 2023',\n",
              " 'date': '05.11.2023',\n",
              " 'n_participants': '3500',\n",
              " 'n_speakers': '6',\n",
              " 'price': 'No information mentioned'}"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Testing\n",
        "We prepared a small dataset for you to test your prompt on.\n",
        "Provided you've written your function, try running the following code.\n",
        "At the end you also have an opportunity to look at the results in a table side-by-side in `with_results.csv`.\n",
        "Your goal is to get at least 60% accuracy, or 26 fields right.\n",
        "\n",
        "Please don't forget to output these metrics, they will be used for grading."
      ],
      "metadata": {
        "id": "QXYAYit8QRQt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "pr_df = pandas.read_csv(\"/content/drive/MyDrive/press_release_extraction.csv\")\n",
        "pr_df.head()"
      ],
      "metadata": {
        "id": "F-b6lWRjS77X",
        "outputId": "4101d691-8534-4842-b03a-8d2b9b2cdbcf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                             pr_text  \\\n",
              "0  InnovAI Summit 2023: A Glimpse into the Future...   \n",
              "1  Press Dispatch: 'Artificial Mariners: Navigati...   \n",
              "2  FOR IMMEDIATE RELEASE\\n\\nAI Innovators Convene...   \n",
              "3  Press Release: Cutting-Edge Innovations Debute...   \n",
              "4  Press Release: Innovative Minds Gather at \"AI ...   \n",
              "\n",
              "                                           pr_parsed  \n",
              "0  {\\n  \"name\": \"InnovAI Summit 2023\",\\n  \"date\":...  \n",
              "1  {\"name\": \"Artificial Mariners: Navigatin' the ...  \n",
              "2  {\"name\": \"Annual Machine Learning Symposium 20...  \n",
              "3  {\"name\": \"AI Advancements Summit\",\\n \"date\": \"...  \n",
              "4  {\"name\": \"AI Horizon 2023\",\\n \"date\": \"October...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ff3aa6ca-64e5-425d-b8ec-c57b175b492a\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>pr_text</th>\n",
              "      <th>pr_parsed</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>InnovAI Summit 2023: A Glimpse into the Future...</td>\n",
              "      <td>{\\n  \"name\": \"InnovAI Summit 2023\",\\n  \"date\":...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Press Dispatch: 'Artificial Mariners: Navigati...</td>\n",
              "      <td>{\"name\": \"Artificial Mariners: Navigatin' the ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>FOR IMMEDIATE RELEASE\\n\\nAI Innovators Convene...</td>\n",
              "      <td>{\"name\": \"Annual Machine Learning Symposium 20...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Press Release: Cutting-Edge Innovations Debute...</td>\n",
              "      <td>{\"name\": \"AI Advancements Summit\",\\n \"date\": \"...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Press Release: Innovative Minds Gather at \"AI ...</td>\n",
              "      <td>{\"name\": \"AI Horizon 2023\",\\n \"date\": \"October...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ff3aa6ca-64e5-425d-b8ec-c57b175b492a')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-ff3aa6ca-64e5-425d-b8ec-c57b175b492a button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-ff3aa6ca-64e5-425d-b8ec-c57b175b492a');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-4cd40aa9-6514-47ff-9950-60b3430c7fa3\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-4cd40aa9-6514-47ff-9950-60b3430c7fa3')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-4cd40aa9-6514-47ff-9950-60b3430c7fa3 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "parsed_list = []\n",
        "fields = {\n",
        "    \"name\": str,\n",
        "    \"date\": str,\n",
        "    \"n_speakers\": int,\n",
        "    \"n_participants\": int,\n",
        "    \"price\": str\n",
        "}\n",
        "correct_fields = 0\n",
        "for row in pr_df.itertuples():\n",
        "    parsed_release = parse_press_release(row.pr_text)\n",
        "    parsed_list.append(json.dumps(parsed_release, indent=4))\n",
        "    golden = json.loads(row.pr_parsed)\n",
        "    for field, field_type in fields.items():\n",
        "        golden_field = golden[field]\n",
        "        parsed_field = parsed_release.get(field)\n",
        "        try:\n",
        "            parsed_field = field_type(parsed_field)\n",
        "        except (ValueError, TypeError):\n",
        "            pass\n",
        "        if golden_field == parsed_field:\n",
        "            correct_fields += 1\n",
        "        else:\n",
        "            print(f\"For {golden['name']} {field} {parsed_release.get(field)} doesn't seem the same as {golden[field]}\")\n",
        "\n",
        "print(correct_fields)"
      ],
      "metadata": {
        "id": "Xu3Yi8wPTQwb",
        "outputId": "951a7a64-1210-4f5f-fe95-b25bf8b9cb40",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For InnovAI Summit 2023 n_speakers 5 doesn't seem the same as 4\n",
            "For Artificial Mariners: Navigatin' the AI Seas date 8.10.2023-9.10.2023 doesn't seem the same as 08.10.2023-09.10.2023\n",
            "For Annual Machine Learning Symposium 2023 date 14.10.2023-16.10.2023 doesn't seem the same as October 14-16, 2023\n",
            "For Annual Machine Learning Symposium 2023 n_participants 2,000 doesn't seem the same as 2000\n",
            "For Annual Machine Learning Symposium 2023 price USD 1,450 doesn't seem the same as USD 1450\n",
            "For AI Advancements Summit name AI Advancements Summit 2023 doesn't seem the same as AI Advancements Summit\n",
            "For AI Horizon 2023 date 15.10.2023 doesn't seem the same as October 15, 2023\n",
            "For Generative Intelligence Conclave, Spain 2023 date 08.10.2023 doesn't seem the same as 8.10.2023\n",
            "For Generative Intelligence Conclave, Spain 2023 price 180 doesn't seem the same as EUR 180\n",
            "26\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pr_df['results'] = parsed_list\n",
        "pr_df.to_csv(\"with_results.csv\")"
      ],
      "metadata": {
        "id": "horwgFpUArhU"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parsed_list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-aCCSWVk3fAh",
        "outputId": "04518f61-881c-40cf-9519-20c099e1bffa"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['{\\n    \"name\": \"InnovAI Summit 2023\",\\n    \"date\": \"05.11.2023\",\\n    \"n_participants\": 3500,\\n    \"n_speakers\": 5,\\n    \"price\": \"No information mentioned\"\\n}',\n",
              " '{\\n    \"name\": \"Artificial Mariners: Navigatin\\' the AI Seas\",\\n    \"date\": \"8.10.2023-9.10.2023\",\\n    \"n_participants\": \"2000\",\\n    \"n_speakers\": 5,\\n    \"price\": \"No information mentioned\"\\n}',\n",
              " '{\\n    \"name\": \"Annual Machine Learning Symposium 2023\",\\n    \"date\": \"14.10.2023-16.10.2023\",\\n    \"n_participants\": \"2,000\",\\n    \"n_speakers\": 4,\\n    \"price\": \"USD 1,450\"\\n}',\n",
              " '{\\n    \"name\": \"AI Advancements Summit 2023\",\\n    \"date\": \"16.10.2023\",\\n    \"n_participants\": \"800\",\\n    \"n_speakers\": \"2\",\\n    \"price\": \"USD 950\"\\n}',\n",
              " '{\\n    \"name\": \"AI Horizon 2023\",\\n    \"date\": \"15.10.2023\",\\n    \"n_participants\": 2000,\\n    \"n_speakers\": \"No information mentioned\",\\n    \"price\": \"No information mentioned\"\\n}',\n",
              " '{\\n    \"name\": \"AI for Equity Summit\",\\n    \"date\": \"15.10.2023\",\\n    \"n_participants\": \"3000\",\\n    \"n_speakers\": \"6\",\\n    \"price\": \"USD 250\"\\n}',\n",
              " '{\\n    \"name\": \"Generative Intelligence Conclave, Spain 2023\",\\n    \"date\": \"08.10.2023\",\\n    \"n_participants\": 2000,\\n    \"n_speakers\": 3,\\n    \"price\": \"\\\\u20ac180\"\\n}']"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ODqkfU_gXq27"
      },
      "source": [
        "# Task 3. Broken telephone\n",
        "\n",
        "3 points\n",
        "\n",
        "In the practice session we saw how to do text-to-speech (TTS) with play-ht. OpenAI API also supports speech-to-text (ASR, automatic speech recognition) using Whisper model. Let's make a broken telephone function\n",
        "\n",
        "```python\n",
        "broken_telephone(message: str, iterations: int = 5) -> str:\n",
        "```\n",
        "\n",
        "which does TTS and ASR a certain amount of times (number equals to `iterations`) and outputs the result.\n",
        "\n",
        "Check it with several initial phrases."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tiktoken"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r-dXV15_y22d",
        "outputId": "c9fd38bb-ca2e-4395-b9fb-d0bd14ef144f"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2023.12.25)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.2.2)\n",
            "Installing collected packages: tiktoken\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed tiktoken-0.5.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of work\n",
        "broken_telephone(\n",
        "    \"A tutor who tooted the flute tried to teach two young tooters to toot.\"\\\n",
        "    \"Said the two to the tutor, Is it harder to toot, or to tutor two tooters to toot?\"\\\n",
        "    \"This year I asked Santa to gift me a little cutie kitten. And my dream came true!\"\n",
        ")"
      ],
      "metadata": {
        "id": "YwN9GNMX3kWo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import json\n",
        "import openai\n",
        "\n",
        "openai.api_key = open(\"/content/drive/MyDrive/.open-ai-api-key.txt\").read().strip()\n",
        "\n",
        "playht_key = open(\"/content/drive/MyDrive/.playht-key.txt\").read().strip()\n",
        "playht_used_id = open(\"/content/drive/MyDrive/.playht-user-id.txt\").read().strip()\n",
        "\n",
        "def generate_speech(text):\n",
        "    response = requests.post(\n",
        "        url=\"https://play.ht/api/v2/tts\",\n",
        "        headers = {\n",
        "            \"AUTHORIZATION\": f\"Bearer {playht_key}\",\n",
        "            \"X-USER-ID\": playht_used_id,\n",
        "            \"accept\": \"text/event-stream\",\n",
        "            \"content-type\": \"application/json\"\n",
        "        },\n",
        "        json = {\n",
        "            \"text\": text,\n",
        "            \"voice\": \"larry\"\n",
        "        }\n",
        "    )\n",
        "    return json.loads(response.text.splitlines()[-2].replace(\"data: \", \"\"))['url']\n",
        "\n",
        "def transcribe_speech(audio_path: str):\n",
        "    audio_file = open(audio_path, \"rb\")\n",
        "    transcript = openai.Audio.transcribe(\"whisper-1\", audio_file)\n",
        "    return transcript['text']\n",
        "\n",
        "def broken_telephone(message: str, iterations: int = 5) -> str:\n",
        "    pass"
      ],
      "metadata": {
        "id": "d9SaGCJM2080"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.auto import tqdm\n",
        "\n",
        "def broken_telephone(message: str, iterations: int = 5) -> str:\n",
        "    current_message = message\n",
        "    for _ in tqdm(range(iterations)):\n",
        "        generated_audio_link = generate_speech(current_message)\n",
        "        with open(\"audio.mp3\", 'wb') as audio_file:\n",
        "            audio_file.write(requests.get(generated_audio_link).content)\n",
        "\n",
        "        transcribed = transcribe_speech(\"audio.mp3\")\n",
        "        if transcribed == current_message:\n",
        "            print(\"No change this iteration\")\n",
        "            break\n",
        "\n",
        "        current_message = transcribed\n",
        "\n",
        "    return current_message"
      ],
      "metadata": {
        "id": "4Rgttqvl4Y6I"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "broken_telephone(\"A tutor who tooted the flute tried to teach two young tooters to toot. Said the two to the tutor, Is it harder to toot, or to tutor two tooters to toot?This year I asked Santa to gift me a little cutie kitten. And my dream came true!\")"
      ],
      "metadata": {
        "id": "HBRLCxnO29L3",
        "outputId": "c0fde51a-d6f4-4711-ff0b-d4f0018fa3fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85,
          "referenced_widgets": [
            "4387e36e40f14a95b2a7ffba22da57b4",
            "ad2acbb1b62e4f8ea0e803db84c6c5e2",
            "41134feaf9d54af98d4670d9b755637b",
            "01dc152ae45e4036ade4fd50bea5079d",
            "b134af65177a4d118e2257ed6a870038",
            "658b4f6a207a43deb5b0160de75b3b3a",
            "5e5f4a8c439941ff8482c104d8bd70b0",
            "88d4bcc059024963ae7ecc28ee1f36cb",
            "c5fead5d7d5246e888c329a64e437db1",
            "cda6a821172b4a9a81be59a1aed5ef4a",
            "6e99c9108b2f48199376a643d8c54693"
          ]
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/5 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4387e36e40f14a95b2a7ffba22da57b4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'A tutor who tooted the flute tried to teach two young tutors to toot, said the tutor to the tutor. Is it harder to tutor? Is it harder to tutor? Is it harder to tutor? Is it harder to tutor? Is it tutor tutors to tooth? This year, I asked Santa to gift me a little kitty kitten, and my dream came true.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note**. Because both algorithms might be too good, you can actually see no change, depending on the sentence you've passed.\n",
        "\n",
        "**Bonus** (1 point). Do the same with the text <-> image translation. You can skip the text <-> audio part if you choose this. However, the authors of the homework don't know any good image captioning (image -> text) API, so you'll have to work with a model locally. It may be done quite conveniently with the [Hugging Face](https://huggingface.co/)  infrastructure, but this is beyond the scope of the first part of the course, so we leave it as an optional exercise."
      ],
      "metadata": {
        "id": "0Vu37PWi3D5n"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OL0uQL48Xq26"
      },
      "source": [
        "# Bonus task.\n",
        "\n",
        "1 point\n",
        "\n",
        "It's quite important to understand the current limitations of the technology. As for LLMs, there are still plenty of weak spots. They can struggle even with such an example:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai\n",
        "!pip install tiktoken"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "plTzRDfM7LSI",
        "outputId": "b408044c-0b28-489d-eb2b-efb1ac97dca3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (0.28.0)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai) (3.9.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2023.11.17)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (23.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.9.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.4.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (4.0.3)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (0.5.2)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2023.6.3)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2023.11.17)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "import openai\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "openai.api_key = open(\"/content/drive/MyDrive/.open-ai-api-key.txt\").read().strip()\n",
        "\n",
        "encoder = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
        "\n",
        "def get_chatgpt_answer(message: str) -> str:\n",
        "    chat_completion = openai.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[{\"role\": \"user\", \"content\": message}]\n",
        "    )\n",
        "    return chat_completion.choices[0].message.content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iwNPpS2XwsD0",
        "outputId": "6311b121-95f7-4231-f221-a3df56d1de12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9In58MpiXq26",
        "outputId": "8e35f839-3dd0-4df9-aa9b-a01e8dd7f5af",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Actual token count: 8\n",
            "ChatGPT thinks it's: There are six tokens in the sentence \"How many tokens are in this sentence?\".\n"
          ]
        }
      ],
      "source": [
        "sentence = \"How many tokens are in this sentence?\"\n",
        "print(f\"Actual token count: {len(encoder.encode(sentence))}\")\n",
        "print(f\"ChatGPT thinks it's: {get_chatgpt_answer(sentence)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Or with this one:"
      ],
      "metadata": {
        "id": "BoxMBnYwTufy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dKo2RPToXq27",
        "outputId": "d247bd5c-6f70-48f4-a616-e79852e4fb67",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "character by character. sentence this reverse this sentence.\n"
          ]
        }
      ],
      "source": [
        "# example\n",
        "sentence = \"Reverse this sentence character by character\"\n",
        "reversed_sentence = get_chatgpt_answer(sentence)\n",
        "print(reversed_sentence[::-1])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LLM doing math\n",
        "\n",
        "Math is also not too easy for LLMs which are getting better at counting and mathematical reasoning thanks to chain-of-thought generation, but can still struggle with symbolic algebra.\n",
        "\n",
        "Let us look at an example:"
      ],
      "metadata": {
        "id": "rnGfrv2A6cDF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"Let's define a mathematical operation x * y := xy + x + y. Is it associative?\"\n",
        "print(get_chatgpt_answer(sentence))"
      ],
      "metadata": {
        "id": "6K-vA6f05_Oe",
        "outputId": "bee3fc00-6fc2-44a4-f7ed-460e3fdf743b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "To determine if the operation x * y is associative, we need to check if (x * y) * z = x * (y * z) holds true for all values of x, y, and z.\n",
            "\n",
            "Using the definition of the operation x * y := xy + x + y, we can calculate:\n",
            "\n",
            "(x * y) * z = ((xy + x + y) * z) = (xyz + xz + yz + x + y + z)\n",
            "\n",
            "x * (y * z) = (x * (yz + y + z)) = (x(yz + y + z) + x + yz + y + z)\n",
            "\n",
            "Expanding both expressions, we get:\n",
            "\n",
            "(x * y) * z = (xyz + xz + yz + x + y + z)\n",
            "\n",
            "x * (y * z) = (xyz + xz + yz + x + y + z)\n",
            "\n",
            "Since (x * y) * z = x * (y * z) for all values of x, y, and z, the operation x * y is associative.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The solution by ChatGPT-4**\n",
        "\n",
        "The\n",
        "\n",
        "To determine if the operation $*$ is associative, we need to check if:\n",
        "\n",
        "$$(a * b) * c = a * (b * c)$$\n",
        "\n",
        "for all real numbers $a$, $b$, and $c$. If this equality holds for all real numbers, then the operation is associative.\n",
        "\n",
        "Given the operation $xy:=xy+x+y$:\n",
        "\n",
        "Calculating $(ab)c$:\n",
        "\n",
        "First,\n",
        "\n",
        "$$ab=ab+a+b.$$\n",
        "\n",
        "Using this result:\n",
        "\n",
        "$$(ab)c=(ab+a+b)c=$$\n",
        "$$=(ab+a+b)c+(ab+a+b)+c=$$\n",
        "$$=abc+ac+bc+ab+a+b+c$$\n",
        "\n",
        "Calculating $a(bc)$:\n",
        "\n",
        "First,\n",
        "\n",
        "$$bc=bc+b+c.$$\n",
        "\n",
        "Using this result:\n",
        "\n",
        "$$a(bc)=a(bc+b+c)=$$\n",
        "$$=a(bc+b+c)+a+bc+b+c=$$\n",
        "$$=abc+ab+ac+a+bc+b+c$$\n",
        "\n",
        "Comparing the two results:\n",
        "\n",
        "$$(ab)c=abc+ac+bc+ab+a+b+c$$\n",
        "\n",
        "$$a(bc)=abc+ab+ac+a+bc+b+c$$\n",
        "\n",
        "The two expressions are not equal for all real numbers $a$, $b$, and $c$. Therefore, the operation $*$ is not associative.\n",
        "\n",
        "**End**"
      ],
      "metadata": {
        "id": "vNSuynxx6tRC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's analyze this solution. You can see that ChatGPT knows definitions and does well with logic, but fails at the very last stage where it can't understand that\n",
        "\n",
        "$$abc+ac+bc+ab+a+b+c$$\n",
        "\n",
        "and\n",
        "\n",
        "$$abc+ab+ac+a+bc+b+c$$\n",
        "\n",
        "is the same expression with permuted summands."
      ],
      "metadata": {
        "id": "2wEDULPl-Phj"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9yqVeXG9Xq27"
      },
      "source": [
        "## Task 4*\n",
        "\n",
        "Find out what is it you are good at, but ChatGPT cannot do. Please try to be objective. Bonus points for analyzing stability of the failures of ChatGPT and their dependence on prompt formulation."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ChatGPT might struggle with certain types of math problems, especially those that require iterative or computational approaches. Here are a few examples of math problems that I can solve using Python, but which might pose a challenge for ChatGPT due to their computational nature:"
      ],
      "metadata": {
        "id": "lFog0Zp73B3W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def factorial(n):\n",
        "    result = 1\n",
        "    for i in range(2, n + 1):\n",
        "        result *= i\n",
        "    return result\n",
        "\n",
        "# Example usage\n",
        "print(factorial(5))  # Output: 120"
      ],
      "metadata": {
        "id": "pTEBjB7gOhhI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c5c3d7b-d13e-4b42-b5e5-31f75166cbad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "120\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"I would like to calculate factorial(5)\"\n",
        "print(get_chatgpt_answer(sentence))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ds-rsaNo3JLD",
        "outputId": "915bd642-c607-4442-ca2c-bbc1ef4e8203"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Factorial is the product of an integer and all the positive integers below it. \n",
            "\n",
            "Factorial of 5 (written as 5!) can be calculated as:\n",
            "5! = 5 * 4 * 3 * 2 * 1\n",
            "   = 120\n",
            "\n",
            "So, factorial(5) = 120\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def fibonacci(n):\n",
        "    a, b = 0, 1\n",
        "    for _ in range(n):\n",
        "        a, b = b, a + b\n",
        "    return a\n",
        "\n",
        "# Example usage\n",
        "print(fibonacci(10))  # Output: 55"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HxCGaf7q3Tec",
        "outputId": "968ef36e-1afa-4e7c-e0d7-e02faf522cc7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "55\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"I would like to calculate fibonacci(10)\"\n",
        "print(get_chatgpt_answer(sentence))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J2LzOoBg3XM1",
        "outputId": "51a4ae17-b86c-4b4f-8148-aee43ddc35dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "To calculate the Fibonacci number at position 10, we can use a simple recursive function or iteration. \n",
            "\n",
            "Using a recursive function:\n",
            "```python\n",
            "def fibonacci(n):\n",
            "    if n <= 1:\n",
            "        return n\n",
            "    else:\n",
            "        return fibonacci(n-1) + fibonacci(n-2)\n",
            "\n",
            "result = fibonacci(10)\n",
            "print(result)\n",
            "```\n",
            "\n",
            "Using iteration:\n",
            "```python\n",
            "def fibonacci(n):\n",
            "    fib = [0, 1]\n",
            "    for i in range(2, n+1):\n",
            "        fib.append(fib[i-1] + fib[i-2])\n",
            "    return fib[n]\n",
            "\n",
            "result = fibonacci(10)\n",
            "print(result)\n",
            "```\n",
            "\n",
            "Both methods will give you the same result, which is 55.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "def estimate_pi(num_samples):\n",
        "    inside_circle = 0\n",
        "    for _ in range(num_samples):\n",
        "        x, y = random.random(), random.random()\n",
        "        if x**2 + y**2 <= 1:\n",
        "            inside_circle += 1\n",
        "    return 4 * inside_circle / num_samples\n",
        "\n",
        "# Example usage\n",
        "print(estimate_pi(10000))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QTqeTNb73adc",
        "outputId": "c45e3f5c-5701-4075-9a7c-8838c870e732"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.1264\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"I would like to calculate Monte Carlo Estimation of \"\n",
        "print(get_chatgpt_answer(sentence))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gM-RHd7_3hNM",
        "outputId": "c4d6d271-ab9d-4366-81bb-68ba01f7de0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "To calculate the Monte Carlo estimation of , you can follow these steps:\n",
            "\n",
            "1. Generate random points within a square: Create a loop that generates a large number of random points within a square with side length 2 (centered at the origin). You can use a random number generator to generate the x and y coordinates of each point. Save the number of points generated as N.\n",
            "\n",
            "2. Count points within the unit circle: For each random point generated in step 1, check if it falls within the unit circle centered at the origin. You can calculate the distance of each point from the origin using the Pythagorean theorem (sqrt(x^2 + y^2)). If the distance is less than or equal to 1, count it as a hit (i.e., within the unit circle).\n",
            "\n",
            "3. Estimate : After generating N random points and counting the number of hits (points within the unit circle), estimate the value of  using the formula:\n",
            "    4 * (number of hits / N)\n",
            "\n",
            "4. Repeat for increased accuracy: To improve the accuracy of the estimation, increase the value of N and repeat steps 1 to 3 multiple times. The more points generated, the closer the estimation will be to the actual value of .\n",
            "\n",
            "Here's a Python example that implements the above steps:\n",
            "\n",
            "```Python\n",
            "import random\n",
            "\n",
            "def estimate_pi(num_points):\n",
            "    num_hits = 0\n",
            "    for _ in range(num_points):\n",
            "        x = random.uniform(-1, 1)\n",
            "        y = random.uniform(-1, 1)\n",
            "        if x**2 + y**2 <= 1:\n",
            "            num_hits += 1\n",
            "    \n",
            "    return 4 * (num_hits / num_points)\n",
            "\n",
            "# Set the number of points\n",
            "num_points = 100000\n",
            "\n",
            "# Estimate pi\n",
            "pi_estimate = estimate_pi(num_points)\n",
            "print(\"Monte Carlo estimation of :\", pi_estimate)\n",
            "```\n",
            "\n",
            "By increasing the value of `num_points`, you should obtain more accurate estimations of  using the Monte Carlo method.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "4387e36e40f14a95b2a7ffba22da57b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ad2acbb1b62e4f8ea0e803db84c6c5e2",
              "IPY_MODEL_41134feaf9d54af98d4670d9b755637b",
              "IPY_MODEL_01dc152ae45e4036ade4fd50bea5079d"
            ],
            "layout": "IPY_MODEL_b134af65177a4d118e2257ed6a870038"
          }
        },
        "ad2acbb1b62e4f8ea0e803db84c6c5e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_658b4f6a207a43deb5b0160de75b3b3a",
            "placeholder": "",
            "style": "IPY_MODEL_5e5f4a8c439941ff8482c104d8bd70b0",
            "value": "100%"
          }
        },
        "41134feaf9d54af98d4670d9b755637b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_88d4bcc059024963ae7ecc28ee1f36cb",
            "max": 5,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c5fead5d7d5246e888c329a64e437db1",
            "value": 5
          }
        },
        "01dc152ae45e4036ade4fd50bea5079d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cda6a821172b4a9a81be59a1aed5ef4a",
            "placeholder": "",
            "style": "IPY_MODEL_6e99c9108b2f48199376a643d8c54693",
            "value": " 5/5 [03:16&lt;00:00, 44.85s/it]"
          }
        },
        "b134af65177a4d118e2257ed6a870038": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "658b4f6a207a43deb5b0160de75b3b3a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5e5f4a8c439941ff8482c104d8bd70b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "88d4bcc059024963ae7ecc28ee1f36cb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c5fead5d7d5246e888c329a64e437db1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cda6a821172b4a9a81be59a1aed5ef4a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6e99c9108b2f48199376a643d8c54693": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}